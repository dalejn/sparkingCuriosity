{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Sparking Curiosity\n",
    "\n",
    "To goal of this notebook is to clean your text to construct a narrative flow of important words, stripped of words that are articles or fillers. Then, the notebook makes a network and animates the narrative flow on that network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python 3"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "import nltk, string, glob\n",
    "import gensim\n",
    "import itertools\n",
    "import re\n",
    "import csv\n",
    "import scipy\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#\n",
    "model = \"/Users/dalezhou/Box/2019-02-neuroDepartments/code/bioASQvectors/bioASQmodel.txt\"\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(model, binary=False, unicode_errors='ignore')\n",
    "\n",
    "#################################################\n",
    "# Initialize, config & define helpful functions #\n",
    "#################################################\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation.replace('-', '')) #filters punctuation except dash\n",
    "lemmatizeCondition = 1\n",
    "lemmatizer = wnl()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Function for finding index of words of interest, like 'references'\n",
    "\n",
    "def find(target):\n",
    "    for i, word in enumerate(sents):\n",
    "        try:\n",
    "            j = word.index(target)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        yield i\n",
    "\n",
    "# Function for handling the input for gensim word2vec\n",
    "\n",
    "class FileToSent(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, 'r'):\n",
    "            ll = line.strip().split(\",\")\n",
    "            ll = [''.join(c for c in s if c not in string.punctuation) for s in ll]\n",
    "            ll = [num.strip() for num in ll]\n",
    "            yield ll\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Read in .txt file(s) from a specified directory #\n",
    "###################################################\n",
    "\n",
    "IDs = glob.glob('/Users/dalezhou/Downloads/psomTest/*.csv')\n",
    "\n",
    "####################\n",
    "# Clean, lemmatize #\n",
    "####################\n",
    "\n",
    "for ID in IDs: # loop through papers\n",
    "    print(ID)\n",
    "    totalWords = []\n",
    "    with open(ID, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            text = row['Answer.Text_Detail']\n",
    "        text = re.sub(\"\\u2013|\\u2014\", \"-\", str(text))  # Replace em-dashes\n",
    "        sents = sent_tokenize(text)  # Split into sentences\n",
    "        sents = [word_tokenize(s) for s in sents]\n",
    "        sents = [[w.translate(translator) for w in s] for s in sents]  # filter punctuation\n",
    "        sents = [[re.sub(r'^[-+]?[0-9]*[\\.\\-]?[0-9]+$', 'numeric', w) for w in s] for s in sents]  # replace all numerals with the holder \"number\"\n",
    "        sents = [[w for w in s if re.search('[^a-zA-Z-0-9-]+', w) is None] for s in sents]  # trips everything but alphanumeric\n",
    "        sents = [[w.lower() for w in s] for s in sents]  # make lower case\n",
    "        sents = [s for s in sents if len(s) > 0]  # remove empty lines\n",
    "        sents = [[w for w in s if not w in stop_words] for s in sents]  # filter stop words\n",
    "        sents = [[w for w in s if len(w) > 1] for s in sents]  # filters out variables, etc\n",
    "        sents = [[w for w in s if len(w) > 2] for s in sents]  # filters out variables, etc\n",
    "        sents = [[w for w in s if len(w) > 3] for s in sents]  # filters out variables and abbreviations\n",
    "        sents = [s for s in sents if len(s) > 0]  # remove empty lines\n",
    "        words = [[lemmatizer.lemmatize(w) for w in s if lemmatizeCondition == 1] for s in sents]  # lemmatize\n",
    "        words = list(itertools.chain.from_iterable(words))  # join list of lists\n",
    "        totalWords.append(words)\n",
    "\n",
    "        model = word_vectors\n",
    "\n",
    "        # get average of all words across years\n",
    "        my_words = list(set(list(itertools.chain.from_iterable(totalWords))))  # append unique words in the whole corpus\n",
    "\n",
    "        # filter out words not in model\n",
    "        my_words = [word for word in my_words if word in model]\n",
    "\n",
    "        # add man and woman to words\n",
    "        my_words.append('man')\n",
    "        my_words.append('woman')\n",
    "\n",
    "        # The number of connections we want: either as a factor of the number of words or a set number\n",
    "        num_top_conns = len(my_words) * 50\n",
    "\n",
    "        # Make a list of all word-to-word distances [each as a tuple of (word1,word2,dist)]\n",
    "        sims = []\n",
    "\n",
    "        # Find similarity distances between each word pair for current year\n",
    "\n",
    "        for i1, word1 in enumerate(my_words):\n",
    "                for i2, word2 in enumerate(my_words):\n",
    "                    if i1 >= i2: continue\n",
    "                    cosine_similarity = model.similarity(word1, word2)\n",
    "                    sim = (word1, word2, cosine_similarity)\n",
    "                    sims.append(sim)\n",
    "\n",
    "        # Sort the list by ascending distance\n",
    "        sims.sort(key=lambda _tuple: _tuple[-1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Animate network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "library(ggraph)\n",
    "library(gganimate)\n",
    "library(igraph)\n",
    "library(RColorBrewer)\n",
    "\n",
    "# Data from http://konect.uni-koblenz.de/networks/sociopatterns-infectious\n",
    "#infect <- read.table('/Users/dalezhou/Downloads/sociopatterns-infectious/out.sociopatterns-infectious', skip = 2, sep = ' ', stringsAsFactors = FALSE)\n",
    "\n",
    "# Code from https://gist.github.com/thomasp85/eee48b065ff454e390e1\n",
    "# https://gist.github.com/jalapic/612036977d9f9c773107681bc4a46d58\n",
    "\n",
    "infect <- read.table('/home/jovyan/networkDynamics.txt', skip = 0, sep = ' ', stringsAsFactors = FALSE)\n",
    "infect <- read.table('/home/jovyan/networkDynamicsLabels.txt', skip = 0, sep = ' ', stringsAsFactors = FALSE)\n",
    "infect$V3 <- NULL \n",
    "#infect$words <- words$V2\n",
    "names(infect) <- c('from', 'to', 'time')\n",
    "infect$timebins <- as.numeric(cut(infect$time, breaks = 500))\n",
    "\n",
    "# We want that nice fading effect so we need to add extra data for the trailing\n",
    "infectAnim <- lapply(1:10, function(i) {infect$timebins  <- infect$timebins + i; infect$delay <- i; infect})\n",
    "infect$delay <- 0\n",
    "infectAnim <- rbind(infect, do.call(rbind, infectAnim))\n",
    "\n",
    "infectGraph <- graph_from_data_frame(infectAnim, directed = F)\n",
    "\n",
    "# We use only original data for the layout\n",
    "subGr <- subgraph.edges(infectGraph, which(E(infectGraph)$delay == 0))\n",
    "V(subGr)$degree <- degree(subGr)\n",
    "V(subGr)$group <- cluster_louvain(subGr)$membership\n",
    "lay <- createLayout(subGr, 'igraph', algorithm = 'fr')\n",
    "\n",
    "# Then we reassign the full graph with edge trails\n",
    "attr(lay, 'graph') <- infectGraph\n",
    "\n",
    "# Now we create the graph with timebins as frame\n",
    "p <- ggraph(data = lay, layout = 'fr', aes(frame = timebins)) + \n",
    "  geom_node_point(size = .1, col = \"white\") +\n",
    "  geom_node_point(aes(alpha=0.6), size = .1, colour = factor(lay$group), show.legend = FALSE) + \n",
    "  # geom_edge_link0(aes(frame = timebins, alpha = delay, width = delay), edge_colour = '#dccf9f') + \n",
    "  geom_edge_link0(aes(frame = timebins, alpha = delay, width = delay, colour = factor(node1.group)), data = gEdges(nodePar = 'group'), show.legend = FALSE) +\n",
    "  # geom_edge_link0(aes(frame = timebins, alpha = delay, width = delay, colour = node1.degree), data = gEdges(nodePar = 'degree'), show.legend = FALSE) +\n",
    "  scale_edge_alpha(range = c(1, 0), guide = 'none') + \n",
    "  scale_edge_width(range = c(0.5, 1.5), trans = 'exp', guide = 'none') + \n",
    "  scale_size(guide = 'none') + \n",
    "  expand_limits(x = c(min(lay$x), max(lay$x)), y = c(min(lay$y), max(lay$y))) +\n",
    "  ggforce::theme_no_axes() + \n",
    "  theme(plot.background = element_rect(fill = '#103fe8'), \n",
    "        panel.background = element_blank(), \n",
    "        panel.border = element_blank(), \n",
    "        plot.title = element_text(color = '#cecece'))\n",
    "\n",
    "# And then we animate\n",
    "animation::ani.options(interval=0.1)\n",
    "# gganimate(p, '/Users/dalezhou/Desktop/Dropbox/service/kamenArt/animation_louvainNodes_coloredSparks_500.gif', title_frame = FALSE)\n",
    "gganim <- gganimate(p, '/home/jovyan/sparkingCuriosity_1600x1600_darkBlue.gif', title_frame = FALSE,\n",
    "         ani.width = 1600, ani.height = 1600, res=300)\n",
    "\n",
    "# to do\n",
    "# add changing text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Python 3",
     "python3",
     "python3",
     "",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "R",
     "ir",
     "R",
     "",
     "r"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.21.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
